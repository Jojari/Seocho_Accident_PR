# -*- coding: utf-8 -*-
"""Traffic_MachineLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_b1LsEtZ9n-xVKk_mVVp658LxWwQLh4c
"""

'''
참고문헌

1. https://www.datacamp.com/community/tutorials/xgboost-in-python
2. http://swlock.blogspot.com/2019/01/xgboost-python.html
3. https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/
4. https://blog.naver.com/bosongmoon/221574787121
5. https://frhyme.github.io/python-lib/plt_hist/
6. https://financedata.github.io/posts/faq_matplotlib_default_chart_size.html
7. https://www.tutorialspoint.com/permutation-and-combination-in-python
'''

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (30,12)
plt.rcParams['lines.linewidth'] = 2
plt.rcParams['lines.color'] = 'Green'
plt.rcParams['axes.grid'] = True

#Google Colab에서 실행 시. 이외 환경에서 실행 시 주석 처리
#인코딩 관련 오류 발생 시, excel에서 Dataset을 UTF-8로 저장 후 코드의 encoding을 'UTF-8'로 바꾸고 재시도하기

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/My Drive/Colab Notebooks/Dataset.csv", encoding = 'EUC-KR')
print(df.columns)

#Google Colab이외의 환경에서 실행 시. Colab에서 실행 시 주석 처리
#인코딩 관련 오류 발생 시, excel에서 Dataset을 UTF-8로 저장 후 코드의 encoding을 'UTF-8'로 바꾸고 재시도하기

'''path = input("데이터셋이 있는 경로(데이터셋명 제외)를 입력하세요 : ")

df = pd.read_csv(path + "/Dataset.csv", encoding = 'EUC-KR')
print(df.columns)'''

#필요없는 컬럼 삭제 및 중복Data 결합_사고건수 기준 학습용(axis=o이면 row, axis=1이면 column / inplace=True는 drop후의 df로 기존 df를 대체한다는 의미, 즉 기본값)

print(df.shape)

df.drop(['NODE_NAME','SI_CODE','GU_CODE','X','Y','Longitude','Latitude','Radius'], axis = 1, inplace = True)
#df = df.drop_duplicates('NODE_ID', keep = 'first')
#drop_duplicate는 필히 변수에 새로 저장해줘야 하고, 그냥 drop은 변수에 저장하면 오히려 에러뜸

df.head(3)
print(df.shape)
print(df.columns)

df.drop(['NODE_ID'], axis = 1, inplace = True)

df.head(3)
print(df.shape)
print(df.columns)

#dtype = object 를 dtype = float로 변환
df[df.columns] = df[df.columns].apply(pd.to_numeric, downcast='float', errors='coerce')
from warnings import simplefilter

#FutureWarning이 눈에 거슬리므로, 출력되지 않도록 한다.

simplefilter(action='ignore', category=FutureWarning)

'''전처리 마지막(저장코드)
data.to_csv("./FinalData.csv")'''

#X는 독립변수, y는 종속변수(사고건수)
#df.iloc[:, n] => n = -1이면 총 사고건수, n = -2이면 차대사람 사고건수, n = -3이면 차대차 사고건수

X,y = df.iloc[:, 1:-3], df.iloc[:, -3]
P,q = df.iloc[:, 1:-3], df.iloc[:, -2]
A,b = df.iloc[:, 1:-3], df.iloc[:, -1]
data_dmatrix_CtoC = xgb.DMatrix(data = X, label = y)
data_dmatrix_CtoP = xgb.DMatrix(data = P, label = q)
data_dmatrix_total = xgb.DMatrix(data = A, label = b)

#무작위로 trainset, testset 뽑아내기

from sklearn.model_selection import train_test_split

#train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)순으로 작성, Validation set이 필요하다면, 기존에 나눈 것을 같은 함수로 한번 더 나눠야

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)
P_train, P_test, q_train, q_test = train_test_split(P, q, test_size = 0.2, random_state = 123)
A_train, A_test, b_train, b_test = train_test_split(A, b, test_size = 0.2, random_state = 123)

#xgboooooooooooooooooost

xg_reg_CtoC = xgb.XGBRegressor(objective = 'reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                         max_depth = 3, alpha = 10, n_estimators = 1000)
xg_reg_CtoP = xgb.XGBRegressor(objective = 'reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                         max_depth = 3, alpha = 10, n_estimators = 1000)
xg_reg_total = xgb.XGBRegressor(objective = 'reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                         max_depth = 3, alpha = 10, n_estimators = 1000)

xg_reg_CtoC.fit(X_train, y_train)
xg_reg_CtoP.fit(P_train, q_train)
xg_reg_total.fit(A_train, b_train)

preds_CtoC = xg_reg_CtoC.predict(X_test)
preds_CtoP = xg_reg_CtoP.predict(P_test)
preds_total = xg_reg_total.predict(A_test)

rmse_CtoC = np.sqrt(mean_squared_error(y_test, preds_CtoC))
rmse_CtoP = np.sqrt(mean_squared_error(q_test, preds_CtoP))
rmse_total = np.sqrt(mean_squared_error(b_test, preds_total))

y_test = list(y_test)
q_test = list(q_test)
b_test = list(b_test)

print("차대차 사고의 RMSE: %f" %(rmse_CtoC))
print("차대사람 사고의 RMSE: %f" %(rmse_CtoP))
print("총 교통사고의 RMSE: %f" %(rmse_total))

print('-'*5 + "차대차" + '-' * 5)
print(xg_reg_CtoC.feature_importances_)
print('-'*16)
for num in range(5):
  print("실제 값: %lf, 예측 값: %lf"%(y_test[num], preds_CtoC[num]))

print('-'*5 + "차대사람" + '-' * 5)
print(xg_reg_CtoP.feature_importances_)
print('-'*18)
for num in range(5):
  print("실제 값: %lf, 예측 값: %lf"%(q_test[num], preds_CtoP[num]))

print('-'*5 + "총 교통사고" + '-' * 5)
print(xg_reg_total.feature_importances_)
print('-'*22)
for num in range(5):
  print("실제 값: %lf, 예측 값: %lf"%(b_test[num], preds_total[num]))

#시각화(1)

xgb.plot_importance(xg_reg_CtoC, title = 'Feature_Importance_in_CtoC')
plt.show()

#시각화(2)

xgb.plot_importance(xg_reg_CtoP, title = 'Feature_Importance_in_CtoP')
plt.show()

#시각화(3)

xgb.plot_importance(xg_reg_total, title = 'Feature_Importance_in_total')
plt.show()

#교차로 반지름(범위)의 히스토그램

r = df.iloc[:, 0]

plt.figure(figsize = (10,6))
plt.hist(r, bins = 20, density = False, cumulative = False, histtype = 'bar',
        orientation = 'vertical', rwidth = 0.8, color = 'g')

df = df.sort_values(by = 'Radius', ascending = False)
print(df.head(5))

#교차로 반지름(범위)의 범주화

sep_r_len = len(df.iloc[:, 0])
r_seperator = int((sep_r_len / 3))

list_r_sep = []

sep_r_first = df.iloc[r_seperator, 0]
sep_r_second = df.iloc[2 * r_seperator, 0]

for jc in df.iloc[:, 0]:
  if jc <= sep_r_first:
    list_r_sep.append(0)
  elif jc <= sep_r_second:
    list_r_sep.append(1)
  else:
    list_r_sep.append(2)


df['Radius'] = list_r_sep
print(df.head(5))
print(df.tail(5))

#사고건수 범주화를 위한 히스토그램 제작

n = int(input("n = -1이면 총 사고건수, n = -2면 차대사람 사고건수, n = -3이면 차대차 사고건수 : "))
x = df.iloc[:, n]

plt.figure(figsize = (10,6))
plt.hist(x, bins = 20, density = False, cumulative = False, histtype = 'bar',
        orientation = 'vertical', rwidth = 0.8, color = 'red')

#차대차(CtoC_num // df.iloc[:, -3]) or 차대사람(CtoP_num // df.iloc[:, -2]) or 전체사고(total_acc // df.iloc[:, -1]) 
#사고건수 및 교차로 범주화를 위한 사고건수 컬럼 정렬 후 

if n == -1:
  standard = 'total_acc'
elif n == -2:
  standard = 'CtoP_num'
elif n == -3:
  standard = 'CtoC_num'
else:
  exit(keep_kernel = True)

df = df.sort_values(by = standard, ascending = False)
print(df.head(5))

sep_len = len(df.iloc[:, n])

# Case 0> df.iloc[:, n] => n = -1이면 총 사고건수, n = -2면 차대사람 사고건수, n = -3이면 차대차 사고건수에 대하여 그룹화를 하지 않고(sep = 0)으로 진행

X,y = df.iloc[:, :-4], df.iloc[:, n]
data_dmatrix = xgb.DMatrix(data = X, label = y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

xg_reg = xgb.XGBRegressor(objective = 'reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                         max_depth = 3, alpha = 10, n_estimators = 1000)

xg_reg.fit(X_train,y_train)
preds = xg_reg.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" %(rmse))
print(xg_reg.feature_importances_)

xgb.plot_importance(xg_reg)
plt.show()

xgb.plot_tree(xg_reg,num_trees=0)
plt.show()

# Case 1> n = -1이면 총 사고건수, n = -2면 차대사람 사고건수, n = -3이면 차대차 사고건수에 대하여 3개 그룹(sep = 3)으로 분류, (주의사항 : dataframe의 index는 필히 int형으로 넣어주어야 하므로, int()를 씌우는 과정 필요!)

seperator = int((sep_len / 3))

list_sep = []

sep_first = df.iloc[seperator, n]
sep_second = df.iloc[2 * seperator, n]

for jc in df.iloc[:, n]:
  if jc <= sep_first:
    list_sep.append(0)
  elif jc <= sep_second:
    list_sep.append(1)
  else:
    list_sep.append(2)


df['acc_legend'] = list_sep
print(df.head(5))
print(df.tail(5))

# Case 1 - Xgboost>
#df.iloc[:, -1] => 'acc_legend'를 의미. 'acc_legend'로 정렬 후 범주화 분석(이미 위에서 선정하였음)

X,y = df.iloc[:, :-4], df.iloc[:, -1]
data_dmatrix = xgb.DMatrix(data = X, label = y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

xg_reg = xgb.XGBRegressor(objective = 'reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                         max_depth = 3, alpha = 10, n_estimators = 1000)

xg_reg.fit(X_train,y_train)
preds = xg_reg.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" %(rmse))
print(xg_reg.feature_importances_)

xgb.plot_importance(xg_reg)
plt.show()

xgb.plot_tree(xg_reg,num_trees=0)
plt.show()

# Case 2> n = -1이면 총 사고건수, n = -2면 차대사람 사고건수, n = -3이면 차대차 사고건수에 대하여 6개 그룹(sep = 6)으로 분류 (사용 안할 케이스의 경우 주석처리 해주기)

seperator = int((sep_len / 6))

df.drop(['acc_legend'], axis = 1, inplace = True)

list_sep = []
print(list_sep)

sep_first = df.iloc[seperator, n]
sep_second = df.iloc[2 * seperator, n]
sep_third = df.iloc[3 * seperator, n]
sep_fourth = df.iloc[4 * seperator, n]
sep_fifth = df.iloc[5 * seperator, n]

for jc in df.iloc[:, n]:
  if jc <= sep_first:
    list_sep.append(0)
  elif jc <= sep_second:
    list_sep.append(1)
  elif jc <= sep_third:
    list_sep.append(2)
  elif jc <= sep_fourth:
    list_sep.append(3)
  elif jc <= sep_fifth:
    list_sep.append(4)
  else:
    list_sep.append(5)

df['acc_legend'] = list_sep
print(df.head(5))
print(df.tail(5))


# Case 2 - Xgboost>
#df.iloc[:, -1] => 'acc_legend'를 의미. 'acc_legend'로 정렬 후 범주화 분석(이미 위에서 선정하였음)

X,y = df.iloc[:, :-4], df.iloc[:, -1]
data_dmatrix = xgb.DMatrix(data = X, label = y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

xg_reg = xgb.XGBRegressor(objective = 'reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                         max_depth = 3, alpha = 10, n_estimators = 1000)

xg_reg.fit(X_train,y_train)
preds = xg_reg.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" %(rmse))
print(xg_reg.feature_importances_)

xgb.plot_importance(xg_reg)
plt.show()

xgb.plot_tree(xg_reg,num_trees=0)
plt.show()

# Case 3> n = -1이면 총 사고건수, n = -2면 차대사람 사고건수, n = -3이면 차대차 사고건수에 대하여 12개 그룹(sep = 12)으로 분류 (사용 안할 케이스의 경우 주석처리 해주기)

seperator = int((sep_len / 12))

df.drop(['acc_legend'], axis = 1, inplace = True)

list_sep = []
print(list_sep)

sep_first = df.iloc[seperator, n]
sep_second = df.iloc[2 * seperator, n]
sep_third = df.iloc[3 * seperator, n]
sep_fourth = df.iloc[4 * seperator, n]
sep_fifth = df.iloc[5 * seperator, n]
sep_sixth = df.iloc[6 * seperator, n]
sep_seventh = df.iloc[7 * seperator, n]
sep_eighth = df.iloc[8 * seperator, n]
sep_nineth = df.iloc[9 * seperator, n]
sep_tenth = df.iloc[10 * seperator, n]
sep_eleventh = df.iloc[11 * seperator, n]

for jc in df.iloc[:, n]:
  if jc <= sep_first:
    list_sep.append(0)
  elif jc <= sep_second:
    list_sep.append(1)
  elif jc <= sep_third:
    list_sep.append(2)
  elif jc <= sep_fourth:
    list_sep.append(3)
  elif jc <= sep_fifth:
    list_sep.append(4)
  elif jc <= sep_sixth:
    list_sep.append(5)
  elif jc <= sep_seventh:
    list_sep.append(6)
  elif jc <= sep_eighth:
    list_sep.append(7)
  elif jc <= sep_nineth:
    list_sep.append(8)
  elif jc <= sep_tenth:
    list_sep.append(9)
  elif jc <= sep_eleventh:
    list_sep.append(10)
  else:
    list_sep.append(11)

df['acc_legend'] = list_sep
print(df.head(5))
print(df.tail(5))


# Case 3 - Xgboost>
#df.iloc[:, -1] => 'acc_legend'를 의미. 'acc_legend'로 정렬 후 범주화 분석(이미 위에서 선정하였음)

X,y = df.iloc[:, :-4], df.iloc[:, -1]
data_dmatrix = xgb.DMatrix(data = X, label = y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

xg_reg = xgb.XGBRegressor(objective = 'reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                         max_depth = 3, alpha = 10, n_estimators = 1000)

xg_reg.fit(X_train,y_train)
preds = xg_reg.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" %(rmse))
print(xg_reg.feature_importances_)

xgb.plot_importance(xg_reg)
plt.show()

xgb.plot_tree(xg_reg,num_trees=0)
plt.show()